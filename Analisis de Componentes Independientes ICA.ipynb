{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de componentes independientes (ICA)\n",
    "\n",
    "El análisis de componentes independientes (ICA) es una técnica estadística que permite identificar fuentes estadísticamente independientes a partir de un conjunto de señales mezcladas. ICA es una técnica que permite descomponer una determinada señal en las fuentes independientes que la componen, utiliza la independencia estadística para identificar las diferentes fuentes que contribuyen a una determinada mezcla de señales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El efecto cocktail party\n",
    "\n",
    "Supongamos que nos encontramos en una reunión, en donde se encuentran varias personas hablando al mismo tiempo (de aquí el nombre Cocktail Party). Nosotros estamos interesados en escuchar solamente a una de las personas que está hablando; lo que nosotros percibimos, es una mezcla de todas las voces y sonidos presentes en la reunión, sin embargo, el cerebro humano tiene la valiosa capacidad de aislar o separar los diferentes sonidos que está escuchando y concentrarse únicamente en la voz que se encuentra interesado.\n",
    "\n",
    "\n",
    "<img src=\"./images/cocktail-party.jpg\" alt=\"SFS\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td align=\"center\">Señal independiente A\n",
    "<img src=\"./images/senal-a.png\" alt=\"Señal A\" width=\"400\"/> </td>\n",
    "        <td align=\"center\">Señal independiente B\n",
    "<img src=\"./images/senal-b.png\" alt=\"Señal b\" width=\"400\"/></td>\n",
    "    </tr>\n",
    "<table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "se mezclan linealmente para generar dos nuevas señales mezcladas M1 y M2\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td align=\"center\">Señal Mezclada M1= A*2.5B\n",
    "<img src=\"./images/senal-mezclada-m1.png\" alt=\"Señal M1\" width=\"400\"/> </td>\n",
    "        <td align=\"center\">Señal Mezclada M2= 0.7A*1.1B\n",
    "<img src=\"./images/senal-mezclada-m2.png\" alt=\"Señal M2\" width=\"400\"/></td>\n",
    "    </tr>\n",
    "<table>\n",
    "    \n",
    "Al aplicar el método ICA a las señales mezcladas M1 y M2, logramos recuperar las dos señales independientes salvo por un factor se escala.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td align=\"center\">Señal Mezclada M1= A*2.5B\n",
    "<img src=\"./images/senal-recuperada-a.png\" alt=\"Señal recuperada A\" width=\"400\"/> </td>\n",
    "        <td align=\"center\">Señal Mezclada M2= 0.7A*1.1B\n",
    "<img src=\"./images/senal-recuperada-b.png\" alt=\"Señal recuperada B\" width=\"400\"/></td>\n",
    "    </tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulación del método ICA\n",
    "\n",
    "\n",
    "Sea ${\\bf{X}} = (x_{1} , ..., x_{d} )$ un vector de $m$ componentes que son observadas asumimos que este vector se produce como una combinación lineal (mezcla) de $m$ señales independientes que denotaremos con el vector $s$, ${\\bf{s}} = (s_{1} , ..., s_{d} )$ es decir, tenemos el sistema:\n",
    "\n",
    "$${\\bf{x_1}} = a_{11}s_1 + a_{12}s_2 + \\cdots + a_{1m}s_m$$\n",
    "$${\\bf{x_2}} = a_{21}s_1 + a_{22}s_2 + \\cdots + a_{2m}s_m$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$${\\bf{x_m}} = a_{m1}s_1 + a_{m2}s_2 + \\cdots + a_{mm}s_m$$\n",
    "\n",
    "\n",
    "Los coeficientes $a_{ij}$ determinan una matriz A que se conoce como matriz de mezcla, el\n",
    "vector x lo llamaremos vector de mezclas y s es el vector de componentes independientes. Podemos representar sistema en la forma:\n",
    "\n",
    "\n",
    "$$\n",
    "{\\bf{x}} = As\n",
    "$$\n",
    "\n",
    "\n",
    "El método ICA consiste en aplicar un algoritmo que nos permita encontrar una matriz de desmezclado $W$ de tal modo que $y = Wx$ sea una buena aproximación del vector $s$.\n",
    "\n",
    "\n",
    "\n",
    "# Condiciones\n",
    "\n",
    "\n",
    "### Independencia estadística\n",
    "\n",
    "Asumiremos para la construcción del método que cada una de las señales si sea estadísticamente independiente respecto\n",
    "a las demás señales.\n",
    "\n",
    "Sea ${\\bf{x}} = (x_{1} , ..., x_{m} )$ un conjunto de variables con función de densidad de probabilidad $f((x_{1} , ..., x_{m} ))$ entonces éstas variables son mutuamente independientes si:\n",
    "\n",
    "$f(x_{1} , ..., x_{m} ) = f_1(x_{1}),f_2(x_{2}),...,f_m(x_{m})$ \n",
    "\n",
    "NOTA: Si las señales no son independientes, se hace uso de PCA\n",
    "\n",
    "### La matriz A debe ser cuadrada\n",
    "\n",
    "El número de fuentes $s_i$ sea igual al número de mezclas $x_i$\n",
    "\n",
    "### Se asume que el experimento está libre de ruido\n",
    "\n",
    "Todas las señales mezcladas $x$ deben tener solamente combinaciones lineales de las señaales independientes $s$, no debe haber información proveniente de ruido externo, ya que éste podría ser interpretado como otra señal independiente, lo cual podría generar interpretaciones incorrectas.\n",
    "\n",
    "\n",
    "### Función de densidad de probabilidad no Gaussiana\n",
    "\n",
    "Las señales $s$ fuente independientes deben tener función de densidad de probabilidad no Gaussiana, de esta forma garantizamos que las componentes independientes efectivamente se pueden separar.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td align=\"center\">Distribución no gausiana\n",
    "<img src=\"./images/no-gausiana.png\" alt=\"No gausiana\" width=\"400\"/> </td>\n",
    "        <td align=\"center\">Distribución gausiana\n",
    "<img src=\"./images/gausiana.png\" alt=\"Gausiana\" width=\"400\"/></td>\n",
    "    </tr>\n",
    "<table>\n",
    "    \n",
    "    \n",
    "    \n",
    "### Los datos deben estar centrados\n",
    "\n",
    "Su media debe ser cero. En general, esta condición no se tiene, para solucionar esto, es necesario hacer un preproceso a los datos que se conoce como _blanqueo_. \n",
    "\n",
    "\n",
    "# Construcción del método ICA\n",
    "\n",
    "\n",
    "\n",
    "## Blanqueo de los datos\n",
    "\n",
    "El preproceso de blanqueo de datos antes del algoritmo del método ICA, ayuda a garantizar\n",
    "la convergencia de este y tambien facilita algunas condiciones para la construcción del algoritmo.\n",
    "\n",
    "\n",
    "Primero, centrar los datos del vector de mezclas $x$, para esto, restamos la media de cada una de las componentes observadas. \n",
    "\n",
    "\n",
    "Después de estoy a partir de la matriz de covarianza de los datos observados, eliminamos la correlación entre\n",
    "cada una de las señales observadas.\n",
    "\n",
    "\n",
    "$$\n",
    "Q =Cov(X)= \\frac{XX^T}{n}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Realizamos ahora la _descomposición espectral_ de la matriz de covarianza para obtener:\n",
    "(explicar)\n",
    "\n",
    "\n",
    "$$\n",
    "Q =EDE^T\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Eliminamos la covarianza entre las señales mezcladas calculando la inversa de la raiz cuadrada de la matriz de covarianza\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{Q} =Q^{-1/2} = ED^{-1/2}E^T\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Al multiplicar nuestro vector de señaales mezcladas por la matriz $D^{-1/2}E^T$, es decir $\\hat{X}=D^{-1/2}E^TX$ eliminamos la covarianza entre cada par de señales que tenemos; si calculamos la matriz de covarianza de los datos blanqueados obtenemos una matriz diagonal:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "Cov(\\hat{X})= \\frac{\\hat{X}\\hat{X}^T}{n}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "Cov(\\hat{X})= \\frac{D^{-1/2}E^TXX^TD^{-1/2}}{n}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "= D^{-1/2}E^T\\frac{XX^T}{n}ED^{-1/2}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "= D^{-1/2}E^TCov(X^T)ED^{-1/2}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "= D^{-1/2}E^TEDE^TED^{-1/2}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "Cov(\\hat{X}) = I_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La covarianza es una medida de correlación, con lo anterior se garantiza que cada una de las señales observadas está no correlacionada con las demás. Aplicando el proceso de blanqueo a los datos obtenemos:\n",
    "\n",
    "\n",
    "<img src=\"./images/datos-blanqueados.png\" alt=\"Datos blanqueados\" width=\"400\"/>\n",
    "\n",
    "\n",
    "### Teorema del límite central\n",
    "(explicar)\n",
    "\n",
    "Por el teorema del limite central, las señales mezcladas $x$ siempre van a ser mas gaussianas que las señales independientes originales $s$\n",
    "\n",
    "\n",
    "*El objetivo es tratar de reducir la gaussianidad que presentan las señales mezcladas $x$ para encontrar las señales independientes $s$*\n",
    "\n",
    "\n",
    "\n",
    "### Negentropia y medida de Nogaussianidad\n",
    "\n",
    "\n",
    "La medida de Gaussianidad está relacionada con la independencia estadística entre las variables, entre mayor sea la Gaussianidad, mayor información comparten las variables y son menos independientes entre si.\n",
    "\n",
    "\n",
    "La entropía se puede considerar como una medida de cantidad de información mutua que contienen los elementos del vector. En base a la entropía $H(y)$ podemos definir la función de negentropía $J$ como:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "J(y) = H(y_{gauss})-H(y)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "donde $y_{gauss}$ es un vector aleatorio Gaussiano con la misma matriz de covarianza de $y$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La negentropía determina la cantidad de información no mutua entre los elementos del vector, es decir a mayor negentropía, menor es la Gaussianidad de los elementos del vector. La negentropía la podemos considerar como una medida de Nogaussianidad.\n",
    "\n",
    "\n",
    "\n",
    "Dado que la esperanza de una variable aleatoria se define como: $E(X)= \\int xf(x)dx$ podemos usar una aproximación de la negentropía a través de la esperanza del vector aleatorio de la siguiente forma\n",
    "\n",
    "\n",
    "$$\n",
    "J(y_i) = c[E(G(y_i))-E(G(V))]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "donde $G$ es una función llamada función de contraste, $c$ es una constante irrelevante y $v$ es una variable Gaussiana con media cero y varianza uno\n",
    "\n",
    "\n",
    "Si hacemos $y_i=w^Tx$, donde $x$ es una de las componentes mezcladas y $w^T$ es el vector de desmezclado para este $x$, nuestra meta será maximizar la función $J$ dada por:\n",
    "\n",
    "\n",
    "$$\n",
    "J(w) = E(G(w^Tx))-E(G(v))\n",
    "$$\n",
    "\n",
    "\n",
    "Ahora podemos generalizar la función anterior para toda la matriz de desmezclado $W$, teniendo en cuenta que se maximizan cada una de las negentropías cuando la suma de todas ellas es máxima. El problema final de maximizacián será el siguiente:\n",
    "\n",
    "\n",
    "$$\n",
    "\\max\\sum_{i=1}^{m} J(w_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bibliografia: http://bdigital.unal.edu.co/46335/1/01830451.2014.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
